<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ActiView Benchmark</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <nav>
        <ul>
            <li><a href="#about">About ActiView</a></li>
            <li><a href="#data">Project Introduction</a></li>
            <li><a href="#leaderboard">Leaderboard</a></li>
	<!-- <li><a href="#bibtex">Citation</a></li> -->
        </ul>
    </nav>

    <header>
        <h1>ACTIVIEW: EVALUATING ACTIVE PERCEPTION ABILITY FOR MULTIMODAL LARGE LANGUAGE MODELS</h1>
<!-- 
        <p>Assessing Multimodal Large Language Models for active perception tasks</p> -->
    </header>
	<div class="social-links">
		<a href="https://github.com/THUNLP-MT/ActiView" target="_blank">
			<img src="figures/github-logo.png" alt="GitHub" class="icon">
			<span>Code</span>
		</a>
		<a href="https://arxiv.org/abs/2410.04659" target="_blank">
			<img src="figures/arxiv-logo.png" alt="arXiv" class="icon">
			<span>ArXiv</span>
		</a>
		<a href="#data" class="cta-button">Learn More</a>
	</div>

    <section id="about">
        <h2>About ActiView</h2>
        <p>
            ActiView is a benchmark designed to evaluate the active perception abilities of Multimodal Large Language Models (MLLMs). In this task, models are challenged to navigate and adjust their filed of view to answer questions, simulating human-like active perception.
        </p>
        <h3>Authors</h3>
        <p>
            Ziyue Wang<sup>1</sup>, Chi Chen<sup>1</sup>, Fuwen Luo<sup>1</sup>, Yurui Dong<sup>2</sup>, Yuanchi Zhang<sup>1</sup>, Yuzhuang Xu<sup>1</sup>, Xiaolong Wang<sup>1</sup>, Peng Li<sup>1</sup>, Yang Liu<sup>1</sup>
        </p>
        <p>
            <sup>1</sup>Tsinghua University, Beijing, China;  <sup>2</sup>Fudan University, Shanghai, China
        </p>
	<h3>Abstract</h3>  
	<p>
	    Active perception, a crucial human capability, involves setting a goal based on the current understanding of the environment and performing actions to achieve that goal. Despite significant efforts in evaluating Multimodal Large Language Models (MLLMs), active perception has been largely overlooked. To address this gap, we propose a novel benchmark named ActiView to evaluate active perception in MLLMs. Since comprehensively assessing active perception is challenging, we focus on a specialized form of Visual Question Answering (VQA) that eases the evaluation yet challenging for existing MLLMs. Given an image, we restrict the perceptual field of a model, requiring it to actively zoom or shift its perceptual field based on reasoning to answer the question successfully. We conduct extensive evaluation over 27 models, including proprietary and open-source models, and observe that the ability to read and comprehend multiple images simultaneously plays a significant role in enabling active perception. Results reveal a significant gap in the active perception capability of MLLMs, indicating that this area deserves more attention. We hope that our benchmark could help develop methods for MLLMs to understand multimodal inputs in more natural and holistic ways.
	</p>
    </section>

    <section id="data">
        <h2>Project introduction</h2>
	<div class="two-column-layout">
	        <div class="left-column">
	            <p>Despite the extensive efforts devoted to MLLM evaluation, active perception remains underexplored. Active perception involves <b>understanding the reasons for sensing, choosing what to perceive, and determining the methods, timing, and locations for achieving that perception</b><sup>[1]</sup>. This is important because in the real world, the desired information often does not appear directly in the center of one’s field of vision. Instead, it requires individuals to move their field of view, locate details, and filter out distracting information. For example, in right figure, suppose we are looking for information in a giant painting. We need to first shift our view to locate the specific area and then possibly zoom in to gather detailed information. Intuitively, <b>active perception not only enables a person or model to accomplish more complex tasks, but it also has the potential to serve as a good indicator of the level of intelligence of a model</b>. This makes it a critical capability that warrants thorough evaluation.
</p>
	        </div>
	        <div class="right-column">
		    <img src="figures/figure1.png" alt="Figure 1: Example of active perception" class="figure">
	        </div>
    	</div>
	<h3>Benchmark</h3>
        <p>Explore the code and datasets for ActiView:</p>
        <a href="https://github.com/THUNLP-MT/ActiView" class="cta-button">View on GitHub</a>
	<h3>Pipeline</h3>
	<h3>Cases</h3>

    </section>

    <section id="leaderboard">
        <h2>Leaderboard</h2>
        <p>results</p>
    </section>

    <section id="reference">
        <h2>References</h2>
        <p>[1] Ruzena Bajcsy, Yiannis Aloimonos, and John K Tsotsos. Revisiting active perception. <i>Autonomous Robots</i>, 42:177–196, 2018.</p>
    </section>
    
    <footer id="contact">
        <h2>Contact Us</h2>
        <p>For questions or inquiries, please reach out to us at: <a href="mailto:w.ziyue1010@gmail.com">w.ziyue1010@gmail.com</a></p>
    </footer>

</body>
</html>
