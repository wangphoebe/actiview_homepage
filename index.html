<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ActiView Benchmark</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <nav>
        <ul>
            <li><a href="#about">About ActiView</a></li>
            <li><a href="#data">Project Introduction</a></li>
            <li><a href="#leaderboard">Leaderboard</a></li>
	<!-- <li><a href="#bibtex">Citation</a></li> -->
        </ul>
    </nav>

    <header>
        <h1>ACTIVIEW: EVALUATING ACTIVE PERCEPTION ABILITY FOR MULTIMODAL LARGE LANGUAGE MODELS</h1>
<!-- 
        <p>Assessing Multimodal Large Language Models for active perception tasks</p> -->
    </header>
	<div class="social-links">
		<a href="https://github.com/THUNLP-MT/ActiView" target="_blank">
			<img src="figures/github-logo.png" alt="GitHub" class="icon">
			<span>Code</span>
		</a>
		<a href="https://arxiv.org/abs/2410.04659" target="_blank">
			<img src="figures/arxiv-logo.png" alt="arXiv" class="icon">
			<span>ArXiv</span>
		</a>
		<a href="" target="_blank">
			<span>ðŸŽ® Demo (coming soon)</span>
		</a>
		<a href="#data" class="cta-button">Learn More</a>
	</div>

    <section id="about">
        <h2>About ActiView</h2>
        <p>
            ActiView is a benchmark designed to evaluate the active perception abilities of Multimodal Large Language Models (MLLMs). In this task, models are challenged to navigate and adjust their filed of view to answer questions, simulating human-like active perception.
        </p>
        <h3>Authors</h3>
        <p>
            Ziyue Wang<sup>1</sup>, Chi Chen<sup>1</sup>, Fuwen Luo<sup>1</sup>, Yurui Dong<sup>2</sup>, Yuanchi Zhang<sup>1</sup>, Yuzhuang Xu<sup>1</sup>, Xiaolong Wang<sup>1</sup>, Peng Li<sup>1</sup>, Yang Liu<sup>1</sup>
        </p>
        <p>
            <sup>1</sup>Tsinghua University, Beijing, China;  <sup>2</sup>Fudan University, Shanghai, China
        </p>
	<h3>Abstract</h3>  
	<p style="text-align: left;">
	    Active perception, a crucial human capability, involves setting a goal based on the current understanding of the environment and performing actions to achieve that goal. Despite significant efforts in evaluating Multimodal Large Language Models (MLLMs), active perception has been largely overlooked. To address this gap, we propose a novel benchmark named ActiView to evaluate active perception in MLLMs. Since comprehensively assessing active perception is challenging, we focus on a specialized form of Visual Question Answering (VQA) that eases the evaluation yet challenging for existing MLLMs. Given an image, we restrict the perceptual field of a model, requiring it to actively zoom or shift its perceptual field based on reasoning to answer the question successfully. We conduct extensive evaluation over 27 models, including proprietary and open-source models, and observe that the ability to read and comprehend multiple images simultaneously plays a significant role in enabling active perception. Results reveal a significant gap in the active perception capability of MLLMs, indicating that this area deserves more attention. We hope that our benchmark could help develop methods for MLLMs to understand multimodal inputs in more natural and holistic ways.
	</p>
    </section>

    <section id="data">
        <h2>Project introduction</h2>
		<div class="two-column-layout">
	        <div class="left-column">
	            <p style="text-align: left;">Despite the extensive efforts devoted to MLLM evaluation, active perception remains underexplored. Active perception involves <b>understanding the reasons for sensing, choosing what to perceive, and determining the methods, timing, and locations for achieving that perception</b><sup>[1]</sup>. This is important because in the real world, the desired information often does not appear directly in the center of oneâ€™s field of vision. Instead, it requires individuals to move their field of view, locate details, and filter out distracting information. For example, in right figure, suppose we are looking for information in a giant painting. We need to first shift our view to locate the specific area and then possibly zoom in to gather detailed information. Intuitively, <b>active perception not only enables a person or model to accomplish more complex tasks, but it also has the potential to serve as a good indicator of the level of intelligence of a model</b>. This makes it a critical capability that warrants thorough evaluation.
</p>
	        </div>
	        <div class="right-column">
		    <img src="figures/figure1.png" alt="Figure 1: Example of active perception" class="figure">
	        </div>
    	</div>
	<h3>Benchmark</h3>
        <p>
		(Explore the code and datasets for ActiView: <a href="https://github.com/THUNLP-MT/ActiView" class="cta-button">View on GitHub</a>)
		</p>
		<img src="figures/cate.png" alt="Figure 2: Example of categories." class="figure" style="width: 80%; height: auto;">
		<p style="text-align: left;">Our benchmark exams active perception abilities of models via different perceptual fields, <b>Acti</b>vely zooming and shifting of <b>View</b>s (<b>ActiView</b>) are required. We summarise zooming and shifting as two fundamental factors of active perception, where we can evaluate active perception abilities of models through the two factors separately or integratedly. ActiView imitates the procedure of active perception by providing models with an initial view, which is a cropped field of the original image, as depicted in the above figure, and requires models to search for missing but important information with view zooming and shifting, and to minimize distractions caused by redundant information in the view.<p> 
		<p style="text-align: left;">When perceiving an image, humans intuitively focus on three principle aspects: the <i>environment</i> depicted in the image, the primary <i>objects</i>, and the <i>event</i> that these objects are engaged in. Correspondingly, we summarise the questions in our benchmark into <i>three main categorises</i>, including <b>environment-centric (Type I)</b>, <b>object-centric (Type II)</b>, <b>and event-centric (Type III)</b> categorises. These main categorises are further divided into <i>eight sub-classes</i> according to the specific type of visual information and visual features used for answering the questions. The name of each sub-classes are shown in the above example and we refer readers to our paper for detailed explanations.
		</p>
	<h3>Pipeline</h3>
		<img src="figures/pipe.png" alt="Figure 3: Evaluation pipelines." class="figure" style="width: 80%; height: auto;">
		<p style="text-align: left;">We design three pipelines for different abilities (as illustrated in the above figure), two fundamental independent abilities, zooming and shifting, and a mixture of them. </p>
		<p style="text-align: left;"><b>Zooming pipeline</b> evaluates the ability to locate and determine fine-grained information that are necessary to answer the question (pipeline (a) above). It contains two stages, the view selection and the question answering stages. To simulate the zooming operation, models are required to first select sub-views to be zoomed given the initial view, then answer questions based on these zoomed views. The initial view used in this pipeline is the full image with size <code>w Ã— h</code>. Each of the selected sub-views will be resized to size <code>w Ã— h</code>, the same size as the initial view.</p>
		<p style="text-align: left;"><b>Shifting pipeline</b> evaluates the ability to shift perceptual fields for missing information and to deduce the answer given perceived perceptual fields, which is also a two-stage pipeline (pipeline (b) above). To simulate the movement of human eyes, models are presented with an initial view, of size <code>w Ã— h</code>, which is a cropped field from the original image, and are asked to determine if the current view or views are sufficient for answering the questions repeatedly until the model is able to answer the question or all the views are consumed.</p>
		<p style="text-align: left;"><b>Mixed pipeline</b> does not specify the type of active perception ability required, while the above pipelines require to either zooming or shifting. As illustrated in pipeline (c), models must independently decide whether to zoom and/or shift to different perceptual fields. Unlike the zooming pipeline, where the model answers questions based on all selected views, in the mixed pipeline, a view would be discarded after zooming if the model recognizes it as irrelevant to the question. In comparison to the shifting pipeline, the mixed pipeline also provides access to the full image view in addition to cropped sub-views. </p>
	<h3>Cases</h3>
		<img src="figures/case.png" alt="Figure 4: Cases." class="figure" style="width: 80%; height: auto;">
		<p style="text-align: left;">We demonstrate three cases for the three proposed pipelines, with results generated by GPT-4 models and Gemini-1.5 models. Case (a) stands for the zooming evaluation, where models successfully identify the view containing useful information and generate the correct result. Case (b) illustrates a failure in the shifting-R evaluation, where all the models continue shifting to new views until all views are used. Though including the correct views, the additional views severely distract the reasoning process, where three out of four employed models produce incorrect answers.</p>
		<p style="text-align: left;">To explore how human-like mixed evaluation affects the visual reasoning process, we further examine this failure case using GPT-4o. As shown in case (c), GPT-4o first zooms into the "upper left" and "upper right" views, then discards the "upper right" view and shifts to the "lower left" one, which finally leads to the correct answer. Notably, in the final preserved views, distracting information (the highest price tag on papaya, "69") is screened out. This indicates that GPT-4o exhibits decent active perception abilities to move the field of view, locate details, and filter out distracting information.</p>
    </section>

    <section id="leaderboard">
        <h2>Leaderboard</h2>
		<div class="table-container">
    		<table border="1" cellpadding="5" cellspacing="0">
    		<caption>The evaluation of active perception abilities on our benchmark, including zooming (for limited resolution scenarios), and shifting (for scenarios of limiting the field of views). "Model AVG": average scores of column "Zooming", "Shifting-R", "Shifting-E", "Shifting-M", and "Shifting-H". The best scores of each column are bolded, and the best scores in each model types are highlighted.</caption>
    		<thead>
    			<tr>
                    <th>Models</th>
                    <th>Full image</th>
                    <th>Zooming</th>
                    <th>Single View</th>
                    <th>Shifting-R</th>
                    <th>Shifting-E</th>
                    <th>Shifting-M</th>
                    <th>Shifting-H</th>
                    <th>Model AVG</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <th colspan="9">Proprietary Models</th>
                </tr>
                <tr>
                    <td><strong>Gemini-1.5-pro</strong></td>
                    <td><strong>73.85</strong></td>
                    <td><strong>72.31</strong></td>
                    <td>58.15</td>
                    <td><strong>67.08</strong></td>
                    <td><strong>67.38</strong></td>
                    <td><strong>65.54</strong></td>
                    <td><strong>67.69</strong></td>
                    <td><strong>68.00</strong></td>
                </tr>
                <tr>
                    <td>GPT-4o</td>
                    <td>67.38</td>
                    <td>68.62</td>
                    <td><strong>61.23</strong></td>
                    <td>67.08</td>
                    <td>66.77</td>
                    <td>65.23</td>
                    <td>64.31</td>
                    <td>66.40</td>
                </tr>
                <tr>
                    <td>Claude 3.5 Sonnet</td>
                    <td>72.92</td>
                    <td>71.69</td>
                    <td>54.46</td>
                    <td>65.23</td>
                    <td>66.15</td>
                    <td>60.31</td>
                    <td>61.85</td>
                    <td>65.05</td>
                </tr>
                <tr>
                    <th colspan="9">Open-source models for multiple images as input</th>
                </tr>
                <tr>
                    <td>Qwen2-VL</td>
                    <td>63.08</td>
                    <td>64.62</td>
                    <td>54.46</td>
                    <td>61.23</td>
                    <td class="highlight">62.77</td>
                    <td class="highlight">64.31</td>
                    <td class="highlight">61.85</td>
                    <td class="highlight">62.96</td>
                </tr>
                <tr>
                    <td>Idefics3-8B-Llama3</td>
                    <td>59.08</td>
                    <td>58.15</td>
                    <td>53.23</td>
                    <td class="highlight">61.85</td>
                    <td>59.38</td>
                    <td>59.69</td>
                    <td>60.31</td>
                    <td>59.88</td>
                </tr>
                <tr>
                    <td>MiniCPM-V 2.6</td>
                    <td>64.62</td>
                    <td>61.85</td>
                    <td>54.46</td>
                    <td>54.77</td>
                    <td>61.23</td>
                    <td>58.15</td>
                    <td>55.69</td>
                    <td>58.34</td>
                </tr>
                <tr>
                    <td>mPLUG-Owl3</td>
                    <td>62.46</td>
                    <td>60.92</td>
                    <td>54.15</td>
                    <td>51.69</td>
                    <td>56.31</td>
                    <td>55.69</td>
                    <td>53.54</td>
                    <td>55.63</td>
                </tr>
                <tr>
                    <td>LLaVA-OneVision</td>
                    <td class="highlight">64.92</td>
                    <td class="highlight">65.23</td>
                    <td class="highlight">56.92</td>
                    <td>53.54</td>
                    <td>57.23</td>
                    <td>52.31</td>
                    <td>48.62</td>
                    <td>55.39</td>
                </tr>
                <tr>
                    <td>InternVL2-8B</td>
                    <td>58.15</td>
                    <td>56.00</td>
                    <td>45.85</td>
                    <td>54.77</td>
                    <td>59.70</td>
                    <td>53.23</td>
                    <td>52.00</td>
                    <td>55.14</td>
                </tr>
                <tr>
                    <td>Mantis</td>
                    <td>59.08</td>
                    <td>60.62</td>
                    <td>52.92</td>
                    <td>52.92</td>
                    <td>55.38</td>
                    <td>52.92</td>
                    <td>52.31</td>
                    <td>54.83</td>
                </tr>
                <tr>
                    <td>Idefics2-8B</td>
                    <td>61.85</td>
                    <td>61.85</td>
                    <td>55.69</td>
                    <td>53.23</td>
                    <td>56.92</td>
                    <td>51.69</td>
                    <td>49.23</td>
                    <td>54.58</td>
                </tr>
                <tr>
                    <td>Brote-IM-XL-3B</td>
                    <td>54.77</td>
                    <td>54.46</td>
                    <td>55.69</td>
                    <td>51.38</td>
                    <td>51.08</td>
                    <td>52.62</td>
                    <td>47.69</td>
                    <td>51.45</td>
                </tr>
                <tr>
                    <td>Idefics2-8B-base</td>
                    <td>52.62</td>
                    <td>48.62</td>
                    <td>47.69</td>
                    <td>49.54</td>
                    <td>50.77</td>
                    <td>47.69</td>
                    <td>47.69</td>
                    <td>48.86</td>
                </tr>
                <tr>
                    <td>Brote-IM-XXL-11B</td>
                    <td>53.85</td>
                    <td>54.77</td>
                    <td>49.23</td>
                    <td>49.85</td>
                    <td>50.77</td>
                    <td>44.92</td>
                    <td>43.69</td>
                    <td>48.80</td>
                </tr>
                <tr>
                    <td>MMICL-XXL-11B</td>
                    <td>51.69</td>
                    <td>49.54</td>
                    <td>50.15</td>
                    <td>49.85</td>
                    <td>49.85</td>
                    <td>46.77</td>
                    <td>45.54</td>
                    <td>48.31</td>
                </tr>
                <tr>
                    <td>MMICL-XL-3B</td>
                    <td>49.85</td>
                    <td>49.85</td>
                    <td>44.31</td>
                    <td>44.92</td>
                    <td>48.92</td>
                    <td>45.85</td>
                    <td>44.31</td>
                    <td>46.77</td>
                </tr>
                <tr>
                    <th colspan="9">Open-source models for single image as input</th>
                </tr>
        		<tr>
                    <td>MiniCPM-Llama3-V-2.5</td>
                    <td>63.87</td>
                    <td>61.25</td>
                    <td class="highlight">54.47</td>
                    <td class="highlight">60.92</td>
                    <td>60.31</td>
                    <td class="highlight">59.38</td>
                    <td class="highlight">58.46</td>
                    <td class="highlight">60.06</td>
                </tr>
                <tr>
                    <td>GLM-4V-9B</td>
                    <td class="highlight">67.08</td>
                    <td>56.92</td>
                    <td>53.85</td>
                    <td>56.92</td>
                    <td class="highlight">60.62</td>
                    <td>56.00</td>
                    <td>52.92</td>
                    <td>56.68</td>
                </tr>
                <tr>
                    <td>InternVL-Vicuna-13B</td>
                    <td>56.92</td>
                    <td>62.77</td>
                    <td>52.31</td>
                    <td>53.85</td>
                    <td>52.92</td>
                    <td>52.92</td>
                    <td>51.08</td>
                    <td>54.71</td>
                </tr>
                <tr>
                    <td>LLaVA-1.6 7B</td>
                    <td>55.08</td>
                    <td class="highlight">68.92</td>
                    <td>50.15</td>
                    <td>51.69</td>
                    <td>52.31</td>
                    <td>49.23</td>
                    <td>48.00</td>
                    <td>54.03</td>
                </tr>
                <tr>
                    <td>InternVL-Vicuna-7B</td>
                    <td>55.38</td>
                    <td>65.23</td>
                    <td>51.70</td>
                    <td>52.92</td>
                    <td>51.38</td>
                    <td>50.77</td>
                    <td>48.62</td>
                    <td>53.78</td>
                </tr>
                <tr>
                    <td>LLaVA-1.6 13B</td>
                    <td>56.92</td>
                    <td>65.23</td>
                    <td>52.31</td>
                    <td>45.85</td>
                    <td>55.08</td>
                    <td>52.62</td>
                    <td>48.92</td>
                    <td>53.54</td>
                </tr>
                <tr>
                    <td>InternVL-Vicuna-13B-448px</td>
                    <td>50.46</td>
                    <td>57.85</td>
                    <td>45.54</td>
                    <td>48.31</td>
                    <td>48.31</td>
                    <td>48.92</td>
                    <td>48.92</td>
                    <td>50.46</td>
                </tr>
                <tr>
                    <td>mPLUG-Owl2-7B</td>
                    <td>55.08</td>
                    <td>55.38</td>
                    <td>52.00</td>
                    <td>47.38</td>
                    <td>46.46</td>
                    <td>46.46</td>
                    <td>46.15</td>
                    <td>48.37</td>
                </tr>
                <tr>
                    <td>Mini-Gemini-7B-HD</td>
                    <td>55.69</td>
                    <td>34.77</td>
                    <td>51.70</td>
                    <td>48.62</td>
                    <td>48.00</td>
                    <td>47.69</td>
                    <td>50.15</td>
                    <td>45.85</td>
                </tr>
                <tr>
                    <td>SEAL</td>
                    <td>48.31</td>
                    <td>54.77</td>
                    <td>42.77</td>
                    <td>42.15</td>
                    <td>42.77</td>
                    <td>40.02</td>
                    <td>40.62</td>
                    <td>44.07</td>
                </tr>
                <tr>
                    <td>Mini-Gemini-7B</td>
                    <td>47.08</td>
                    <td>17.85</td>
                    <td>47.38</td>
                    <td>39.38</td>
                    <td>38.15</td>
                    <td>38.15</td>
                    <td>36.00</td>
                    <td>33.91</td>
                </tr>
                </tbody>
                </table>
		</div>


    </section>

    <section id="reference">
        <h2>References</h2>
        <p style="text-align: left;">[1] Ruzena Bajcsy, Yiannis Aloimonos, and John K Tsotsos. Revisiting active perception. <i>Autonomous Robots</i>, 42:177â€“196, 2018.</p>
    </section>
    
    <footer id="contact">
        <h2>Contact Us</h2>
        <p>For questions or inquiries, please reach out to us at: <a href="mailto:w.ziyue1010@gmail.com">w.ziyue1010@gmail.com</a></p>
    </footer>

</body>
</html>
